{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "import pymorphy2\n",
    "import string\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "from collections import Counter\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(data, mode):\n",
    "    if mode == 'collection':\n",
    "        tokens = ' '.join(data).split()\n",
    "    elif mode == 'doc':\n",
    "        tokens = data.split()\n",
    "    else:\n",
    "        raise ValueError('invalid mode')\n",
    "    collection_length = len(tokens)\n",
    "    collection_model = Counter(tokens)\n",
    "    for key in collection_model.keys():\n",
    "        collection_model[key] /= collection_length\n",
    "    return collection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_prob(query, lambda_coef, collection_model, doc_model):\n",
    "    p = 1\n",
    "    valid_prob = False\n",
    "    for token in query.split():\n",
    "        if token in collection_model:\n",
    "            valid_prob = True\n",
    "            collection_prob = collection_model[token]\n",
    "            if token in doc_model:\n",
    "                doc_prob = doc_model[token]\n",
    "            else:\n",
    "                doc_prob = collection_prob\n",
    "            p *= ((1 - lambda_coef) * collection_prob + lambda_coef * doc_prob)\n",
    "    if valid_prob:\n",
    "        return p\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = ['Калифорнийский посёлок был независим от США почти 3 месяца пока не пришла пора праздновать День независимости и закупать алкоголь',\n",
    "        'В наполеоновской артиллерии служили три генерала из одной семьи',\n",
    "        'Совершивший первое кругосветное путешествие на велосипеде ездил по России на коне']\n",
    "normalized_facts = []\n",
    "for i in range(len(facts)):\n",
    "    tokens = facts[i].split()\n",
    "    for j in range(len(tokens)):\n",
    "        tokens[j] = morph.parse(tokens[j])[0].normal_form\n",
    "    normalized_facts.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized facts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "калифорнийский посёлок быть независимый от сша почти 3 месяц пока не пришлый пора праздновать день независимость и закупать алкоголь \n",
      "\n",
      "в наполеоновский артиллерия служить три генерал из один семья \n",
      "\n",
      "совершить первое кругосветный путешествие на велосипед ездить по россия на кон \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fact in normalized_facts:\n",
    "    print(fact, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = open('data.txt', encoding='utf8').read()\n",
    "docs = sent_tokenize(docs)\n",
    "normalized_docs = []\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = regex.sub(' ', docs[i])\n",
    "    docs[i] = docs[i][:-1]\n",
    "    tokens = docs[i].split()\n",
    "    for j in range(len(tokens)):\n",
    "        tokens[j] = morph.parse(tokens[j])[0].normal_form     \n",
    "    tokens_without_stop_words = []\n",
    "    stop_POS = ['PREP', 'CONJ', 'PRCL', 'NPRO']\n",
    "    for token in tokens:\n",
    "        POS = morph.parse(token)[0].tag.POS\n",
    "        if POS not in stop_POS and POS is not None:\n",
    "            tokens_without_stop_words.append(token)\n",
    "    normalized_docs.append(' '.join(tokens_without_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_model = get_model(normalized_docs, 'collection')\n",
    "doc_models = [get_model(doc, 'doc') for doc in normalized_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact 1\n",
    "Калифорнийский посёлок был независим от США почти 3 месяца пока не пришла пора праздновать День независимости и закупать алкоголь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почтовая служба США требовала от жителей для упрощения своей работы сменить название их посёлка на Раф или Рэди но те отказались \n",
      "-----------\n",
      "\n",
      "В телесериале Дни в Долине Смерти о независимости и скорой обратной зависимости Раф энд Рэди рассказывается в двух эпизодах  Rough and Ready    и Birthright    \n",
      "-----------\n",
      "\n",
      "Из достопримечательностей в Раф энд Рэди остались три здания х годов постройки ежегодно апреля отмечается собственный день независимости \n",
      "-----------\n",
      "\n",
      "Независимое государство с апреля по июля года \n",
      "-----------\n",
      "\n",
      "Более того в ближайшем городе Невада Сити старателям отказались продавать алкоголь так как они иностранцы \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = {}\n",
    "lambda_coef = 0.5\n",
    "for i in range(len(doc_models)):\n",
    "    probs[i] = get_query_prob(normalized_facts[0], lambda_coef, collection_model, doc_models[i])\n",
    "\n",
    "top_sentences_index = np.array(sorted(probs.items(), key=operator.itemgetter(1), reverse=True)[:5])[:, 0].astype(int)\n",
    "for index in top_sentences_index:\n",
    "    print(docs[index], '\\n-----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact 2\n",
    "В наполеоновской артиллерии служили три генерала из одной семьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оба его сына были генералами артиллеристами Наполеоновской армии \n",
      "-----------\n",
      "\n",
      "Первый генеральный инспектор артиллерии \n",
      "-----------\n",
      "\n",
      "Во время Ста дней все три представителя семьи Д’Абовиль отказались активно поддерживать Наполеона \n",
      "-----------\n",
      "\n",
      "Семья Д’Абовилей существует до сих пор и является одной из самых многочисленных среди сохранившихся семей французской знати \n",
      "-----------\n",
      "\n",
      "Огюстен Габриэль Д’Абовиль французский военачальник эпохи Наполеоновских войн бригадный генерал военный артиллерист \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = {}\n",
    "lambda_coef = 0.5\n",
    "for i in range(len(doc_models)):\n",
    "    probs[i] = get_query_prob(normalized_facts[1], lambda_coef, collection_model, doc_models[i])\n",
    "\n",
    "top_sentences_index = np.array(sorted(probs.items(), key=operator.itemgetter(1), reverse=True)[:5])[:, 0].astype(int)\n",
    "for index in top_sentences_index:\n",
    "    print(docs[index], '\\n-----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact 3\n",
    "Совершивший первое кругосветное путешествие на велосипеде ездил по России на коне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Томас Стивенс  человек совершивший первое кругосветное путешествие на велосипеде проехав  миль и пробыв в пути более двух лет  с апреля по декабрь года  \n",
      "-----------\n",
      "\n",
      "На всём протяжении своего кругосветного путешествия Стивенс отправлял свои путевые заметки в Harper’s Magazine которые позже вышли в двухтомнике с страницами Вокруг света на велосипеде \n",
      "-----------\n",
      "\n",
      "Первое трансатлантическое путешествие завершилось августа года продлившись всего  дня без учёта вынужденных остановок из за плохой погоды и прочих неурядиц \n",
      "-----------\n",
      "\n",
      "Позже Томас устроился на работу в шахту в Колорадо где ему пришла идея о путешествии на велосипеде по стране \n",
      "-----------\n",
      "\n",
      "Оставив велосипед в подземном хранилище Стивенс поездом добрался до Лондона чтобы договориться об условиях своего путешествия в Европе и Азии \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = {}\n",
    "lambda_coef = 0.5\n",
    "for i in range(len(doc_models)):\n",
    "    probs[i] = get_query_prob(normalized_facts[2], lambda_coef, collection_model, doc_models[i])\n",
    "\n",
    "top_sentences_index = np.array(sorted(probs.items(), key=operator.itemgetter(1), reverse=True)[:5])[:, 0].astype(int)\n",
    "for index in top_sentences_index:\n",
    "    print(docs[index], '\\n-----------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
